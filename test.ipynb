{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sliti-wassim/Storage/anaconda3/envs/paper/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchtune.modules.attention import CausalSelfAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtune.modules import RotaryPositionalEmbeddings\n",
    "from torch import nn\n",
    "\n",
    "# Assuming head_dim = 72 (as per your input tensor shape)\n",
    "\n",
    "pos_embeddings = RotaryPositionalEmbeddings(dim=head_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 72\n",
    "head_dim = 72 // 12\n",
    "num_heads = 12\n",
    "q_proj = nn.Linear(embed_dim, embed_dim)\n",
    "k_proj = nn.Linear(embed_dim, embed_dim)\n",
    "v_proj = nn.Linear(embed_dim, embed_dim)\n",
    "output_proj = nn.Linear(embed_dim, embed_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_layer = CausalSelfAttention(\n",
    "            embed_dim=embed_dim,\n",
    "            num_heads=num_heads,\n",
    "            num_kv_heads=num_heads,  # Use the same number of heads for key/value\n",
    "            head_dim=head_dim,  # Correctly use head_dim here\n",
    "            q_proj=q_proj,\n",
    "            k_proj=k_proj,\n",
    "            v_proj=v_proj,\n",
    "            output_proj=output_proj,\n",
    "            pos_embeddings=pos_embeddings,\n",
    "            attn_dropout=0.1\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand([4,24,56,72])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 24, 56, 72])\n"
     ]
    }
   ],
   "source": [
    "x_shape = x.shape\n",
    "output = attention_layer(x.view(x.shape[0] * x.shape[1], -1, embed_dim))\n",
    "output = output.view(x_shape)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[4, 24, 28, 72]' is invalid for input of size 387072",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 113\u001b[0m\n\u001b[1;32m    100\u001b[0m causal_self_attention \u001b[38;5;241m=\u001b[39m CausalSelfAttention(\n\u001b[1;32m    101\u001b[0m     embed_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4032\u001b[39m,\n\u001b[1;32m    102\u001b[0m     num_heads\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m56\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    109\u001b[0m     pos_embeddings\u001b[38;5;241m=\u001b[39mpos_embeddings,\n\u001b[1;32m    110\u001b[0m )\n\u001b[1;32m    112\u001b[0m \u001b[38;5;66;03m# Apply the CausalSelfAttention to the input tensor\u001b[39;00m\n\u001b[0;32m--> 113\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mcausal_self_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28mprint\u001b[39m(output\u001b[38;5;241m.\u001b[39mshape)  \u001b[38;5;66;03m# Expected output shape: [4, 24, 4032]\u001b[39;00m\n",
      "File \u001b[0;32m~/Storage/anaconda3/envs/paper/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Storage/anaconda3/envs/paper/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[19], line 55\u001b[0m, in \u001b[0;36mCausalSelfAttention.forward\u001b[0;34m(self, x, mask, input_pos)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# Reshape queries, keys, and values to include heads\u001b[39;00m\n\u001b[1;32m     54\u001b[0m q \u001b[38;5;241m=\u001b[39m q\u001b[38;5;241m.\u001b[39mview(bsz, seq_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)  \u001b[38;5;66;03m# [4, 24, 56, 72]\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m k \u001b[38;5;241m=\u001b[39m \u001b[43mk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbsz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_kv_heads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhead_dim\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [4, 24, 28, 72]\u001b[39;00m\n\u001b[1;32m     56\u001b[0m v \u001b[38;5;241m=\u001b[39m v\u001b[38;5;241m.\u001b[39mview(bsz, seq_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_kv_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)  \u001b[38;5;66;03m# [4, 24, 28, 72]\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# Apply positional embeddings to queries and keys\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[4, 24, 28, 72]' is invalid for input of size 387072"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from torchtune.modules import RotaryPositionalEmbeddings\n",
    "from torchtune.modules.kv_cache import KVCache\n",
    "from typing import Optional\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_dim: int,\n",
    "        num_heads: int,\n",
    "        num_kv_heads: int,\n",
    "        head_dim: int,\n",
    "        q_proj: nn.Module,\n",
    "        k_proj: nn.Module,\n",
    "        v_proj: nn.Module,\n",
    "        output_proj: nn.Module,\n",
    "        pos_embeddings: nn.Module,  # Pass the RotaryPositionalEmbeddings here\n",
    "        kv_cache: Optional[KVCache] = None,\n",
    "        max_seq_len: int = 4096,\n",
    "        attn_dropout: float = 0.0,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.num_kv_heads = num_kv_heads\n",
    "        self.embed_dim = embed_dim\n",
    "        self.attn_dropout = attn_dropout\n",
    "        self.head_dim = head_dim\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "        # Set layers\n",
    "        self.kv_cache = kv_cache\n",
    "        self.q_proj = q_proj\n",
    "        self.k_proj = k_proj\n",
    "        self.v_proj = v_proj\n",
    "        self.output_proj = output_proj\n",
    "        self.pos_embeddings = pos_embeddings  # Store the positional embeddings\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: Tensor,\n",
    "        *,\n",
    "        mask: Optional[Tensor] = None,\n",
    "        input_pos: Optional[Tensor] = None,\n",
    "    ) -> Tensor:\n",
    "        bsz, seq_len, _ = x.shape\n",
    "\n",
    "        # Project input to queries, keys, and values\n",
    "        q = self.q_proj(x)\n",
    "        k = self.k_proj(x)\n",
    "        v = self.v_proj(x)\n",
    "\n",
    "        # Reshape queries, keys, and values to include heads\n",
    "        q = q.view(bsz, seq_len, self.num_heads, self.head_dim)  # [4, 24, 56, 72]\n",
    "        k = k.view(bsz, seq_len, self.num_kv_heads, self.head_dim)  # [4, 24, 28, 72]\n",
    "        v = v.view(bsz, seq_len, self.num_kv_heads, self.head_dim)  # [4, 24, 28, 72]\n",
    "\n",
    "        # Apply positional embeddings to queries and keys\n",
    "        q = self.pos_embeddings(q, input_pos=input_pos)\n",
    "        k = self.pos_embeddings(k, input_pos=input_pos)\n",
    "\n",
    "        # Transpose for scaled dot-product attention\n",
    "        q = q.transpose(1, 2)  # [4, 56, 24, 72]\n",
    "        k = k.transpose(1, 2)  # [4, 28, 24, 72]\n",
    "        v = v.transpose(1, 2)  # [4, 28, 24, 72]\n",
    "\n",
    "        # Update key-value cache\n",
    "        if self.kv_cache is not None:\n",
    "            k, v = self.kv_cache.update(input_pos, k, v)\n",
    "\n",
    "        if mask is not None:\n",
    "            mask = mask[:, None, :, :]\n",
    "\n",
    "        # Perform scaled dot-product attention\n",
    "        output = nn.functional.scaled_dot_product_attention(\n",
    "            q,\n",
    "            k,\n",
    "            v,\n",
    "            attn_mask=mask,\n",
    "            dropout_p=self.attn_dropout,\n",
    "            is_causal=self.kv_cache is None and mask is None,\n",
    "        )\n",
    "\n",
    "        # Reshape the output to be the same shape as the input\n",
    "        output = output.transpose(1, 2).contiguous().view(bsz, seq_len, -1)\n",
    "        return self.output_proj(output)\n",
    "\n",
    "# Example input tensor of shape [batch_size, seq_length, embed_dim]\n",
    "input_tensor = torch.randn(4, 24, 4032)  # [4, 24, 56 * 72]\n",
    "\n",
    "# Define the projection layers and the output projection layer\n",
    "q_proj = nn.Linear(4032, 4032)\n",
    "k_proj = nn.Linear(4032, 4032)\n",
    "v_proj = nn.Linear(4032, 4032)\n",
    "output_proj = nn.Linear(4032, 4032)\n",
    "\n",
    "# Initialize the RotaryPositionalEmbeddings and CausalSelfAttention\n",
    "pos_embeddings = RotaryPositionalEmbeddings(dim=72)\n",
    "\n",
    "causal_self_attention = CausalSelfAttention(\n",
    "    embed_dim=4032,\n",
    "    num_heads=56,\n",
    "    num_kv_heads=28,  # GQA configuration with 28 key/value heads\n",
    "    head_dim=72,\n",
    "    q_proj=q_proj,\n",
    "    k_proj=k_proj,\n",
    "    v_proj=v_proj,\n",
    "    output_proj=output_proj,\n",
    "    pos_embeddings=pos_embeddings,\n",
    ")\n",
    "\n",
    "# Apply the CausalSelfAttention to the input tensor\n",
    "output = causal_self_attention(input_tensor)\n",
    "print(output.shape)  # Expected output shape: [4, 24, 4032]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[4, 24, 28, 72]' is invalid for input of size 387072",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 24\u001b[0m\n\u001b[1;32m     11\u001b[0m causal_self_attention \u001b[38;5;241m=\u001b[39m CausalSelfAttention(\n\u001b[1;32m     12\u001b[0m     embed_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m56\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m72\u001b[39m,\n\u001b[1;32m     13\u001b[0m     num_heads\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m56\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     20\u001b[0m     pos_embeddings\u001b[38;5;241m=\u001b[39mpos_embeddings,\n\u001b[1;32m     21\u001b[0m )\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Apply the CausalSelfAttention to the input tensor\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mcausal_self_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Storage/anaconda3/envs/paper/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Storage/anaconda3/envs/paper/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[17], line 53\u001b[0m, in \u001b[0;36mCausalSelfAttention.forward\u001b[0;34m(self, x, mask, input_pos)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# Reshape queries, keys, and values to include heads\u001b[39;00m\n\u001b[1;32m     52\u001b[0m q \u001b[38;5;241m=\u001b[39m q\u001b[38;5;241m.\u001b[39mview(bsz, seq_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\n\u001b[0;32m---> 53\u001b[0m k \u001b[38;5;241m=\u001b[39m \u001b[43mk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbsz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_kv_heads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhead_dim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m v \u001b[38;5;241m=\u001b[39m v\u001b[38;5;241m.\u001b[39mview(bsz, seq_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_kv_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# Apply positional embeddings to queries and keys\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[4, 24, 28, 72]' is invalid for input of size 387072"
     ]
    }
   ],
   "source": [
    "# Example input tensor of shape [batch_size, seq_length, embed_dim]\n",
    "input_tensor = torch.randn(4, 24, 56 * 72)  # [4, 24, 4032]\n",
    "\n",
    "# Define the projection layers and the output projection layer\n",
    "q_proj = nn.Linear(56 * 72, 56 * 72)\n",
    "k_proj = nn.Linear(56 * 72, 56 * 72)\n",
    "v_proj = nn.Linear(56 * 72, 56 * 72)\n",
    "output_proj = nn.Linear(56 * 72, 56 * 72)\n",
    "\n",
    "# Initialize the CausalSelfAttention module\n",
    "causal_self_attention = CausalSelfAttention(\n",
    "    embed_dim=56 * 72,\n",
    "    num_heads=56,\n",
    "    num_kv_heads=28,  # For example, if you want GQA\n",
    "    head_dim=72,\n",
    "    q_proj=q_proj,\n",
    "    k_proj=k_proj,\n",
    "    v_proj=v_proj,\n",
    "    output_proj=output_proj,\n",
    "    pos_embeddings=pos_embeddings,\n",
    ")\n",
    "\n",
    "# Apply the CausalSelfAttention to the input tensor\n",
    "output = causal_self_attention(input_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
